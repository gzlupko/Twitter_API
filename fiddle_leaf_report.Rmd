---
title: "fiddle_leaf_report"
author: "Gian Zlupko"
date: "4/1/2021"
output: html_document
---

```{r setup, include=FALSE, echo = FALSE}
library(ROAuth) 
library(twitteR) 
library(dplyr) 
library(ggplot2) 

api_key <- "7KhDdJQseh65sfG5dN4A6Nlve"

api_secret <- "MKyYAyDyIDdDXZQLNXz6sPpyrxWFqqeOXIpXqrTzxfUIzv0ILs" 

access_token <- "363179894-yJNsEk5RErQLKDw5SdzF1jK94phSHwJirzziZUi8"

access_token_secret <- "avP6VT9yMEmW2lYYF04hybVl61sjHpUztCX3cuBb3wk6a"

setup_twitter_oauth(api_key, api_secret, access_token, access_token_secret) 
```




## Download Tweets 

Searching Twitter for the keyword, 'fiddle leaf fig', referring to the popular houseplant. Fiddle leaf figs are exploding in popularity today. After mining the fiddle leaf data from Twitter, I format tweet activity about fiddle leaf figs for data analysis. 


```{r, echo = FALSE}

TL <- searchTwitter("fiddle leaf fig", n= 50, since='2021-04-01', until='2021-04-07', lang = "en")#Make sure you change the dates here to be 6 days from today.

TL <- do.call("rbind", lapply(TL, as.data.frame))


```


## Explore Data 

1. Top posts by favorites 
2. Accounts with most retweets 
3. Accounts with most favorites 

```{r, echo = FALSE}

write.csv(TL, "raw_fiddle_leaf_tweets.csv") 

favorites <- TL %>% 
  select(screenName, text, favoriteCount) %>%
  arrange(desc(favoriteCount))  %>%
  top_n(10)   

ggplot(data = favorites, aes(x = screenName, y = favoriteCount)) + geom_bar(stat = "identity") + xlab("Screen Name") + ylab("Favorite Count") + ggtitle("Tweets with Most Favorites by Account") 

   

```



Accounts posting most fidle leaf activity recently 

```{r, echo = FALSE}

TL %>%
  group_by(screenName) %>%
  count(screenName) %>%
  arrange(desc(n)) %>%
  top_n(15) 


```



## Text Cleaning

Common terms: Which terms appeared in fiddle leaf tweets more than 5 times? 


```{r}


library(tm) 
#Remove the htlm tags from your text
TL <- gsub("<.*?>", "", TL)
TL <- gsub("nbsp", "" , TL)
TL <- gsub("nbspnbspnbsp", "" , TL)
TL <- gsub("<U+00A0><U+00A0><U+00A0>", "" , TL)



  
corpus <- VCorpus(VectorSource(TL)) 
corpus <- tm_map(corpus, stripWhitespace)
#Convert to lower case
corpus <- tm_map(corpus, tolower)
#Remove pre-defined stop words ('the', 'a', etc)
corpus <- tm_map(corpus, removeWords, stopwords('english'))
#Convert words to stems ("education" = "edu") for analysis
corpus <- tm_map(corpus, stemDocument)
#Remove numbers
corpus <- tm_map(corpus, removeNumbers)
#remove punctuation
corpus <- tm_map(corpus, removePunctuation)
#Convert to plain text for mapping by wordcloud package
corpus <- tm_map(corpus, PlainTextDocument, lazy = TRUE)
#Convert corpus to a term document matrix - so each word can be analyzed individuallly

corpus <- tm_map(corpus, removeWords, c("false")) 


tdm.corpus <- TermDocumentMatrix(corpus)


  
word_count <- sort(rowSums(as.matrix(tdm.corpus)), decreasing=TRUE)

word_count <- rowSums(as.matrix(tdm.corpus))
word_count<- subset(word_count, word_count >=5)
word_count <- data.frame(term = names(word_count), freq = word_count)

word_count %>%
  select(freq) %>%
  arrange(desc(freq)) %>%
  top_n(10) 



```



## Text Visualization

```{r}
head(word_count) 


ggplot(data = word_count, aes(x = term, y = freq)) + geom_bar(stat = "identity") 

```






Clustering 








Note that the `echo = FALSE` parameter was added to the code chunk to prevent printing of the R code that generated the plot.
